networks:
  snowplow-network:
    driver: bridge

volumes:
  postgres_data:
  spark-logs:
  hadoop_namenode:
  hadoop_datanode1:

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports: ["2181:2181"]
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks: [snowplow-network]

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
    ports: ["9092:9092"]
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks: [snowplow-network]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: snowplow
      POSTGRES_USER: snowplow
      POSTGRES_PASSWORD: snowplow123
    ports: ["5432:5432"]
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./Python-Django-Blog-Website/sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U snowplow -d snowplow"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks: [snowplow-network]

  snowplow-collector:
    image: snowplow/scala-stream-collector-kafka:2.10.0
    command: ["--config", "/snowplow/config/collector.hocon"]
    volumes:
      - ./Python-Django-Blog-Website/config/collector.hocon:/snowplow/config/collector.hocon:ro
    environment:
      JVM_OPTS: "-Xmx512m -Xms512m -XX:+UseG1GC -XX:+UseStringDeduplication"
    ports: ["8080:8080"]
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks: [snowplow-network]

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_DAEMON_MEMORY: 1g
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      SPARK_MASTER_OPTS: "-Dspark.deploy.recoveryMode=NONE -Dspark.ui.host=0.0.0.0"
      KAFKA_BOOTSTRAP: kafka:9092
    ports:
      - "18081:8080"
      - "7077:7077"
    volumes:
      - ./Python-Django-Blog-Website/spark-apps:/opt/bitnami/spark/jobs
      - ./Python-Django-Blog-Website/spark-data:/opt/bitnami/spark/data
      - spark-logs:/opt/bitnami/spark/logs
      - ./Python-Django-Blog-Website/spark-config:/opt/bitnami/spark/conf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://spark-master:8080"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks: [snowplow-network]

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2g
      SPARK_EXECUTOR_MEMORY: 1800m
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_WEBUI_PORT: 8081
      KAFKA_BOOTSTRAP: kafka:9092
    depends_on:
      spark-master:
        condition: service_healthy
    ports: ["18082:8081"]
    volumes:
      - ./Python-Django-Blog-Website/spark-apps:/opt/bitnami/spark/jobs
      - ./Python-Django-Blog-Website/spark-data:/opt/bitnami/spark/data
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped
    networks: [snowplow-network]

  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2g
      SPARK_EXECUTOR_MEMORY: 1800m
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_WEBUI_PORT: 8082
      KAFKA_BOOTSTRAP: kafka:9092
    depends_on:
      spark-master:
        condition: service_healthy
    ports: ["18083:8082"]
    volumes:
      - ./Python-Django-Blog-Website/spark-apps:/opt/bitnami/spark/jobs
      - ./Python-Django-Blog-Website/spark-data:/opt/bitnami/spark/data
      - spark-logs:/opt/bitnami/spark/logs
    restart: unless-stopped
    networks: [snowplow-network]

  snowplow-enrich:
    image: snowplow/snowplow-enrich-kafka:3.8.0
    container_name: snowplow-enrich
    command: ["--config", "/snowplow/config/enrich.hocon", "--iglu-config", "/snowplow/config/iglu_resolver.json", "--enrichments", "/snowplow/config/enrichments"]
    volumes:
      - ./Python-Django-Blog-Website/config/enrich.hocon:/snowplow/config/enrich.hocon
      - ./Python-Django-Blog-Website/config/iglu_resolver.json:/snowplow/config/iglu_resolver.json
      - ./Python-Django-Blog-Website/config/enrichments:/snowplow/config/enrichments
    environment:
      JVM_OPTS: "-Xmx512m -Xms512m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxRAMPercentage=75.0"
      JAVA_OPTS: "-Djava.security.egd=file:/dev/./urandom"
    depends_on:
      kafka:
        condition: service_healthy
      snowplow-collector:
        condition: service_healthy
    restart: always
    networks: [snowplow-network]
    deploy:
      resources:
        limits:
          memory: 768M
          cpus: "1.0"
        reservations:
          memory: 512M
          cpus: "0.5"

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_namenode
    hostname: namenode
    networks:
      - snowplow-network
    environment:
      - CLUSTER_NAME=snowplow
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9000:9000"
      - "9870:9870"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode1
    hostname: datanode
    networks:
      - snowplow-network
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SERVICE_PRECONDITION=namenode:9000
    ports:
      - "9864:9864"
      - "9866:9866"
      - "9867:9867"
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    depends_on:
      - namenode

  # Django-based E-commerce app
  ecommerce:
    build:
      context: ./E-commerce-Project-1
      dockerfile: Dockerfile
    volumes:
      - ./E-commerce-Project-1:/app
    expose:
      - "8000"
    environment:
      SNOWPLOW_COLLECTOR_URL: http://snowplow-collector:8080
    depends_on:
      - postgres
      - snowplow-collector
    networks: [snowplow-network]

  # Django-based Blog app
  blog:
    build:
      context: ./Python-Django-Blog-Website
      dockerfile: Dockerfile
    volumes:
      - ./Python-Django-Blog-Website:/app
    expose:
      - "8000"
    environment:
      SNOWPLOW_COLLECTOR_URL: http://snowplow-collector:8080
    depends_on:
      - postgres
      - snowplow-collector
    networks: [snowplow-network]

  # Django-based Bookstore app
  book:
    build:
      context: ./django-ecommerce-bookstore-master
      dockerfile: Dockerfile
    volumes:
      - ./django-ecommerce-bookstore-master:/app
    expose:
      - "8000"
    environment:
      SNOWPLOW_COLLECTOR_URL: http://snowplow-collector:8080
    depends_on:
      - postgres
      - snowplow-collector
    networks: [snowplow-network]

  # Flask-based Search app (or Django if that's the case)
  search:
    build:
      context: ./simple-search
      dockerfile: Dockerfile
    volumes:
      - ./simple-search:/app
    expose:
      - "8000"
    environment:
      SNOWPLOW_COLLECTOR_URL: http://snowplow-collector:8080
    depends_on:
      - snowplow-collector
    networks: [snowplow-network]

  nginx:
    image: nginx:latest
    container_name: nginx
    ports:
      - "80:80"
    volumes:
      - ./Python-Django-Blog-Website/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./E-commerce-Project-1/shop/static:/static/ecommerce:ro
      - ./Python-Django-Blog-Website/staticfiles:/static/blog:ro
      - ./django-ecommerce-bookstore-master/staticfiles:/static/book:ro
      - ./simple-search/static:/static/search:ro
    depends_on:
      - ecommerce
      - search
      - book
      - blog
    networks: [snowplow-network]
